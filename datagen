#!/usr/bin/env python3
# Generate random datasets with xarray and Dask, easiest way
# Then can test some very simple operations across several different methods
import os
import numpy as np
import dask.array as da
import xarray as xr
import datetime
import sys

# Check
try:
    reso = float(sys.argv[-1])
except ValueError:
    raise ValueError('Must pass grid resolution convertible to float.')
print(f'Resolution: {reso}')

# Dimensions
print('Initial stuff.')
dir = '60lev'
dtype = np.float32
step = 2
if dir=='1lev':
    ntime = 1000
    plev = np.array([500.0], dtype=dtype)
elif dir=='60lev':
    ntime = 200
    plev = np.linspace(0, 1013.25, 61, dtype=dtype)
    plev = (plev[1:] + plev[:-1])/2
else:
    raise ValueError('Unknown data dir {dir}.')
time = np.arange(1/step, ntime, dtype=dtype)/step
lat = np.arange(-90, 90 + reso/2, reso, dtype=dtype)
lat = (lat[1:] + lat[:-1])/2
lon = np.arange(-180, 180 - reso/2, reso, dtype=dtype)
shape = (ntime, plev.size, lat.size, lon.size)
# Variables
params = ('u', 'v', 't')
params_attrs = {
    'u': {'long_name':'zonal wind', 'units':'m/s'},
    'v': {'long_name':'meridional wind', 'units':'m/s'},
    't': {'long_name':'temperature', 'units':'degC'}
    }
# Coordinates
coords = ('time', 'plev', 'lat', 'lon')
coords_values = {
    'time': time,
    'plev': plev,
    'lat': lat,
    'lon': lon,
    }
coords_attrs = {
    'time': {'long_name':'time', 'calendar':'360_day', 'units':'days since 00-01-01 00:00:00', 'axis':'T'},
    'plev': {'long_name':'pressure level', 'units':'hPa', 'axis':'Z'},
    'lat': {'long_name':'latitude',  'units':'degN', 'axis':'Y'},
    'lon': {'long_name':'longitude', 'units':'degE', 'axis':'X'},
    }
for attrs in coords_attrs.values():
    attrs['standard_name'] = attrs['long_name']

# Variables
# Numpy is extremely slow at making tons of random variables, so need to
# use dask so generation is done in parallel and we don't get memory overload
# NOTE: Random routines (including numpy ones) don't accept chunks.
print('Making variables.')
scale = 10
offset = 0.5
# chunks = (1, 1, *shape[2:]) # one chunk per horizontal slice, works for bigger datasets
# chunks = (1, shape[1], shape[2], shape[3]) # one chunk per timestep worked *way* better
chunks = (1, shape[1]//4, shape[2], shape[3]) # compromise?
params_map = {name: (coords, (scale*da.random.normal(0, 1, shape, chunks=chunks)).cumsum(axis=0).astype(dtype), params_attrs[name]) for name in params}
coords_map = {name: ((name,), coords_values[name], coords_attrs[name]) for name in coords}
print('Making dataset.')
data = xr.Dataset(params_map, coords_map)
for param in data.variables.values():
    param.encoding.update({'_FillValue':None}) # disable default fill value

# Save
# NOTE: NaN encoding causes issues with NCO
# Diabled follwoing instructions here: http://xarray.pydata.org/en/stable/io.html
# NOTE: Can reorder variables following directions here: https://github.com/pydata/xarray/issues/479
# For some reason coordinates not first by default
# NOTE: Try both NetCDF3 classic and NetCDF4. Xarray has good guide
# on difference, found here: http://xarray.pydata.org/en/stable/generated/xarray.Dataset.to_netcdf.html
# Hunch is that some tools may do much better with full NetCDF4.
# format = 'NETCDF3_CLASSIC'
format = 'NETCDF4' # may fail for some CDO versions, try to fix
formats = {3:'NETCDF3_CLASSIC', 4:'NETCDF4'}
formats = {3:'NETCDF3_CLASSIC'} # differences are minor and 'classic' is still everywhere, so let's just use this
for num,format in formats.items():
    out = f'{dir}/dataN{lat.size:04d}T{ntime}_{num}.nc'
    if os.path.exists(out):
        os.remove(out)
    print(f'Writing to {out}.')
    data[[*coords, *params]].to_netcdf(out, mode='w', format=format, unlimited_dims={'time':True})

