#!/usr/bin/env bash
################################################################################
# For *this task*, we get the the eddy heat flux and eddy momentum flux
# Use this to call successively call the different task1.x files, then time
# them, so we can compare their output. Consider automatically appending
# results to a markdown table or something
################################################################################
# Helper function
# We require a subshell to redirect 'time' for some reason; because it's weird
# builtin or something
name=$0
dir=$1 # directories where data is stored
[ -z "$dir" ] && echo "Error: Please enter location of NetCDFs for globbing/testing." && exit 1
! [ -d "$dir" ] && echo "Error: Directory \"$dir\" not found." && exit 1
output=logs/fluxes_${dir}_${HOSTNAME%%.*}.log # store here
header="| nlat | size (version) | name | real (s) | user (s) | sys (s) |\n| --- | --- | --- | --- | --- | --- |\n"
rm $output 2>/dev/null
seconds() {
  # Convert time command output to raw seconds
  m=${1%m*}
  s=${1#*m}
  s=${s%.*}
  t=${1#*.} # tail
  echo $((m*60 + s)).${t%s} # converts e.g. 1m35.23s to 85.23s
}
bench() {
  # Run time command
  res=$( (time ${@:2}) 2>&1 )
  echo $1
  echo $res
  array=($res)
  # Get result, tabulate it, add it to markdown file
  real=$(seconds ${array[1]})
  user=$(seconds ${array[3]})
  sys=$(seconds ${array[5]})
  printf "| $nlat | $size ($version) | $1 | **$real** | $user | $sys |\n" >>$output
}

# Loop over datasets with different resolutions
datas=($dir/data*.nc)
counter=0
for data in ${datas[@]}; do
  # Header
  # NOTE: The 'disk usage' often very different from the 'apparent size'
  # ls -l; former depends on internals of how data is stored, and on Cheyenne,
  # ends up way bigger. See: https://unix.stackexchange.com/a/106278/112647
  # Explanation of why this happens: https://serverfault.com/a/290091/427991
  size=$(command du -h $data | xargs | cut -d' ' -f1)
  version=${data%.nc}
  version=${version##*_}
  nlat=${data#*N}
  nlat=$(printf '%.0f' ${nlat%T*})
  echo && echo "Dataset: $data ($size)"
  printf "\n$header" >>$output # add header for each new file

  # Julia
  bench "Julia + PackageCompiler" ./precompyle/${name} $data
  # bench "Julia (naive)" julia -e 'push!(LOAD_PATH, "./"); using fluxes; fluxes.eddy_flux("'${name}'")' # will be ridiculously slow, don't bother

  # Python with NetCDF4
  bench "NetCDF4" python ${name}_nc4.py $data 0

  # Python with xarray method
  bench "XArray + no dask" python ${name}_xarray.py $data 0

  # Python with xarray and Dask chunking by 2D slice
  # bench "XArray + 100-timestep chunks" python ${name}.py $data 100
  if [ $dir == 1lev ]; then
    bench "XArray + 1000 t chunks" python ${name}_xarray.py $data 1000
    # bench "XArray + 200 t chunks"  python ${name}_xarray.py $data 200
    bench "XArray + 100 t chunks" python ${name}_xarray.py $data 100
    # bench "XArray + 50 t chunks"   python ${name}_xarray.py $data 50
    # bench "XArray + 20 t chunks"   python ${name}_xarray.py $data 20
    bench "XArray + 10 t chunks" python ${name}_xarray.py $data 20
  elif [ $dir == 60lev ]; then
    bench "XArray + 200 t chunks" python ${name}_xarray.py $data 200
    # bench "XArray + 50 t chunks"  python ${name}_xarray.py $data 50
    bench "XArray + 20 t chunks" python ${name}_xarray.py $data 20
    # bench "XArray + 10 t chunks"  python ${name}_xarray.py $data 10
    # bench "XArray + 5 t chunks"   python ${name}_xarray.py $data 5
    bench "XArray + 2 t chunks" python ${name}_xarray.py $data 2
    # bench "XArray + 1 t chunks"   python ${name}_xarray.py $data 1
  else
    echo "Error: Unknown chunk combos for sample data in dir \"$dir\"."
    exit 1
  fi

  # Fortran method
  # Super-duper low level

  # CDO method
  # NOTE: On mac just run with serial IO
  # NOTE: Serial IO turned out to be really small difference
  bench "CDO" ${name}.cdo $data
  # bench "CDO no expr" ${name}_stupid.cdo $data
  # bench "CDO + serial IO" ${name}_stupid.cdo $data '-L'

  # NCL method
  # NOTE: NCL needs special dyld library path but so does brew, screws up
  # Homebrew if we set it, so set it locally
  export DYLD_LIBRARY_PATH="/usr/local/lib/gcc/4.9"
  bench "NCL" ncl -Q -n "filename=\"$data\"" "large=\"0\"" ${name}.ncl
  # bench "NCL + LargeFile" ncl -Q -n "filename=\"$data\"" "large=\"1\"" ${name}.ncl
  export DYLD_LIBRARY_PATH=""

  # NCO method with NCAP
  # Add options for handling large files; see: http://nco.sourceforge.net/nco.html#Temporary-Output-Files
  # No need to do both; we just need to 'open' or 'create' in RAM to prevent
  # issues with overwrites (2nd example). In this case we aren't writing to the
  # same file so we don't need to worry about those options.
  # NOTE: Turned out no tmp file made pretty much zero difference
  bench "NCO" ${name}.nco $data
  # bench "NCO + no tmp file" ${name}.nco $data --no_tmp_fl
  # bench "NCO + open in RAM" ${name}.nco $data --open_ram --no_tmp_fl
  # bench "NCO + create in RAM" ${name}.nco $data --create_ram --no_tmp_fl

done
